{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EN.605. 646 01 : Natural Language Processing \n",
    "# Lab 2 (Tim Chen)\n",
    "## (a) Simple Character LM\n",
    "\n",
    "Disclaimer: I am using ChatGPT to assist the results analysis.\n",
    "\n",
    "(e.g. markdown table format & insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import assets.charlm as lm\n",
    "\n",
    "mylm = lm.train_char_lm('assets/subtitles.txt', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n', 0.9940436161014506),\n",
      " (' ', 0.00220962628494572),\n",
      " ('.', 0.0013930252665962147),\n",
      " (',', 0.0009607070804111826),\n",
      " ('?', 0.0003362474781439139),\n",
      " (\"'\", 0.00024017677010279565),\n",
      " ('u', 0.00019214141608223654),\n",
      " ('\"', 0.0001441060620616774),\n",
      " ('s', 0.0001441060620616774),\n",
      " ('-', 9.607070804111827e-05),\n",
      " ('!', 4.8035354020559135e-05),\n",
      " (':', 4.8035354020559135e-05),\n",
      " ('m', 4.8035354020559135e-05),\n",
      " ('p', 4.8035354020559135e-05),\n",
      " ('r', 4.8035354020559135e-05)]\n",
      "[('n', 0.8), ('e', 0.1), ('s', 0.1)]\n",
      "[('r', 0.9992144540455616), ('s', 0.0007855459544383347)]\n"
     ]
    }
   ],
   "source": [
    "lm.print_probs(mylm, 'atio')\n",
    "lm.print_probs(mylm, 'nivi')\n",
    "lm.print_probs(mylm, 'supe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 10 samples of 80 characters\n",
    "samples = [lm.generate_text(mylm, 4, 80) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "I remembered?\n",
      "They welcomes these alled you're politary.\n",
      "It's Smart must...\n",
      "THEN\n",
      "---\n",
      "Syll was shirty-six his sudden.\n",
      "- Moskowitzer body else if you mean in Africa of\n",
      "---\n",
      "I'll be suck around like what?\n",
      "Yeah you play way, he's just lear the swim?\n",
      "Pete \n"
     ]
    }
   ],
   "source": [
    "for s in [samples[4], samples[8], samples[9]]:\n",
    "    print('---')\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Calculate perplexitysamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity of The student loves homework    :  3.76\n",
      "perplexity of The yob loves homework        :   inf\n",
      "perplexity of It is raining in London       :  3.06\n",
      "perplexity of asdfjkl; qwerty               :   inf\n"
     ]
    }
   ],
   "source": [
    "perplexity_samples = [\n",
    "    \"The student loves homework\",\n",
    "    \"The yob loves homework\",\n",
    "    \"It is raining in London\",\n",
    "    \"asdfjkl; qwerty\"\n",
    "]\n",
    "for s in perplexity_samples:\n",
    "    print(f\"perplexity of {s:<30}: {lm.perplexity(s, mylm, 4):5.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)  Naive  smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothed perplexity of The student loves homework    :       3.76\n",
      "smoothed perplexity of The yob loves homework        :      37.28\n",
      "smoothed perplexity of It is raining in London       :       3.06\n",
      "smoothed perplexity of asdfjkl; qwerty               :  106907.17\n"
     ]
    }
   ],
   "source": [
    "for s in perplexity_samples:\n",
    "    print(f\"smoothed perplexity of {s:<30}: {lm.smoothed_perplexity(s, mylm, 4):10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Language Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order 0\n",
      "Testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e60d5e0dfd0410aa3d612fbca547e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- first line ---\n",
      "char model:\n",
      "da ->      29.32\n",
      "de ->      29.52\n",
      "en ->      31.44\n",
      "fr ->      21.57\n",
      "it ->      23.48\n",
      "nl ->      26.63\n",
      "------------------\n",
      "--- first line ---\n",
      "word model:\n",
      "da ->  465944.01\n",
      "de ->  443230.07\n",
      "en ->  533520.54\n",
      "fr ->    1237.39\n",
      "it ->  159425.51\n",
      "nl ->   95920.13\n",
      "------------------\n",
      "Accuracy of char model:\n",
      "da -> 0.96\n",
      "de -> 0.93\n",
      "en -> 0.95\n",
      "fr -> 0.97\n",
      "it -> 0.98\n",
      "nl -> 0.91\n",
      "Accuracy of word model:\n",
      "da -> 1.00\n",
      "de -> 1.00\n",
      "en -> 1.00\n",
      "fr -> 1.00\n",
      "it -> 1.00\n",
      "nl -> 0.99\n",
      "Order 2\n",
      "Testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2556bb3717474d3285fceeece1d69f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- first line ---\n",
      "char model:\n",
      "da ->      51.43\n",
      "de ->      45.13\n",
      "en ->      35.40\n",
      "fr ->       8.01\n",
      "it ->      34.80\n",
      "nl ->      19.05\n",
      "------------------\n",
      "--- first line ---\n",
      "word model:\n",
      "da -> 2610157.22\n",
      "de -> 2610157.22\n",
      "en -> 2610157.22\n",
      "fr ->  183722.52\n",
      "it -> 2610157.22\n",
      "nl -> 2610157.22\n",
      "------------------\n",
      "Accuracy of char model:\n",
      "da -> 1.00\n",
      "de -> 1.00\n",
      "en -> 1.00\n",
      "fr -> 1.00\n",
      "it -> 1.00\n",
      "nl -> 0.99\n",
      "Accuracy of word model:\n",
      "da -> 0.97\n",
      "de -> 0.90\n",
      "en -> 0.97\n",
      "fr -> 0.98\n",
      "it -> 0.93\n",
      "nl -> 0.91\n",
      "Order 4\n",
      "Testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bc675857dc43049a3a820d1caa346c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- first line ---\n",
      "char model:\n",
      "da ->  129237.70\n",
      "de ->   46355.61\n",
      "en ->   15225.68\n",
      "fr ->       6.25\n",
      "it ->   16784.92\n",
      "nl ->    2581.62\n",
      "------------------\n",
      "--- first line ---\n",
      "word model:\n",
      "da ->  837677.64\n",
      "de ->  837677.64\n",
      "en ->  837677.64\n",
      "fr ->  609733.15\n",
      "it ->  837677.64\n",
      "nl ->  837677.64\n",
      "------------------\n",
      "Accuracy of char model:\n",
      "da -> 1.00\n",
      "de -> 1.00\n",
      "en -> 1.00\n",
      "fr -> 1.00\n",
      "it -> 1.00\n",
      "nl -> 0.99\n",
      "Accuracy of word model:\n",
      "da -> 0.92\n",
      "de -> 0.77\n",
      "en -> 0.80\n",
      "fr -> 0.83\n",
      "it -> 0.75\n",
      "nl -> 0.81\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "languages = ['da', 'de', 'en', 'fr', 'it', 'nl']\n",
    "lang_files = {lang: f'assets/{lang}.train.txt' for lang in languages}\n",
    "\n",
    "for order in [0, 2, 4]:\n",
    "    models = {\"char\": None, \"word\": None}\n",
    "    models[\"char\"] = {lang: lm.train_char_lm(lang_file, order) for lang, lang_file in lang_files.items()}\n",
    "    models[\"word\"] = {lang: lm.train_word_lm(lang_file, order) for lang, lang_file in lang_files.items()}\n",
    "\n",
    "    print(f\"Order {order}\")\n",
    "    res_lang = {\"char\": {lang:[] for lang in languages}, \"word\": {lang:[] for lang in languages}}\n",
    "    with open('assets/test.txt') as test_file:\n",
    "        print(f\"Testing...\")\n",
    "        for i, line in enumerate(tqdm(test_file, total=1200)):\n",
    "            language, text = line.split('\\t')\n",
    "            for mode in [\"char\", \"word\"]:\n",
    "                scores = {lang: lm.smoothed_perplexity(text, models[mode][lang], order, mode=mode) for lang in languages}\n",
    "                predicted_language = min(scores, key=scores.get)\n",
    "                if i == 0:\n",
    "                    print(\"---\", \"first line\", \"---\")\n",
    "                    print(f\"{mode} model:\")\n",
    "                    for lang in scores:\n",
    "                        print(f\"{lang:<3}-> {scores[lang]:10.2f}\")\n",
    "                    print(\"------------------\")\n",
    "                # Compare predicted_language with the actual language and count accuracy\n",
    "                res_lang[mode][language].append(predicted_language == language)\n",
    "\n",
    "    for mode in [\"char\", \"word\"]:\n",
    "        print(f\"Accuracy of {mode} model:\")\n",
    "        for lang in languages:\n",
    "            print(f\"{lang} -> {sum(res_lang[mode][lang]) / len(res_lang[mode][lang]):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "| Model Type | n-gram Order | da   | de   | en   | fr   | it   | nl   |\n",
    "|------------|--------------|------|------|------|------|------|------|\n",
    "| Char       | 0            | 0.96 | 0.93 | 0.95 | 0.97 | 0.98 | 0.91 |\n",
    "| Char       | 2            | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.99 |\n",
    "| Char       | 4            | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.99 |\n",
    "| Word       | 0            | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.99 |\n",
    "| Word       | 2            | 0.97 | 0.90 | 0.97 | 0.98 | 0.93 | 0.91 |\n",
    "| Word       | 4            | 0.92 | 0.77 | 0.80 | 0.83 | 0.75 | 0.81 |\n",
    "\n",
    "### Analysis:\n",
    "\n",
    "1. Order 0 (Unigram): The word model outperforms the character model across all languages, reaching near-perfect accuracy.\n",
    "2. Order 2 (Trigram): The character model reaches near-perfect accuracy for all languages. The word model's performance drops slightly for some languages, particularly German (de) and Dutch (nl).\n",
    "3. Order 4 (5-gram): The character model maintains near-perfect accuracy. However, the word model's performance further declines for all languages.\n",
    "\n",
    "The higher accuracy of character models with increasing n-grams suggests they might be more resilient to data sparsity issues compared to word models at higher n-gram orders. The word model's decreasing accuracy at higher orders indicates challenges with overfitting or handling unseen word combinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Gender Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order 0\n",
      "Testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0d28f3c3264c0a9193264b3af1e196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- first line ---\n",
      "char model:\n",
      "M  ->      25.65\n",
      "F  ->      25.62\n",
      "------------------\n",
      "--- first line ---\n",
      "word model:\n",
      "M  ->     955.55\n",
      "F  ->     940.71\n",
      "------------------\n",
      "Accuracy of char model:\n",
      "M -> 0.45\n",
      "F -> 0.61\n",
      "Accuracy of word model:\n",
      "M -> 0.57\n",
      "F -> 0.68\n",
      "Order 2\n",
      "Testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b5e26fd73e4f96a7f76944dce892ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- first line ---\n",
      "char model:\n",
      "M  ->      18.30\n",
      "F  ->      18.00\n",
      "------------------\n",
      "--- first line ---\n",
      "word model:\n",
      "M  ->   12420.65\n",
      "F  ->   12979.05\n",
      "------------------\n",
      "Accuracy of char model:\n",
      "M -> 0.58\n",
      "F -> 0.71\n",
      "Accuracy of word model:\n",
      "M -> 0.63\n",
      "F -> 0.50\n",
      "Order 4\n",
      "Testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491246a3ddd4445392911b5313f06c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- first line ---\n",
      "char model:\n",
      "M  ->      30.97\n",
      "F  ->      34.85\n",
      "------------------\n",
      "--- first line ---\n",
      "word model:\n",
      "M  ->  256502.09\n",
      "F  ->  256502.09\n",
      "------------------\n",
      "Accuracy of char model:\n",
      "M -> 0.64\n",
      "F -> 0.60\n",
      "Accuracy of word model:\n",
      "M -> 0.71\n",
      "F -> 0.33\n"
     ]
    }
   ],
   "source": [
    "genders_train_file = \"assets/tennis.train.txt\"\n",
    "genders_test_file = \"assets/tennis.test.txt\"\n",
    "genders = [\"M\", \"F\"]\n",
    "genders_files = {\"M\": \"assets/tennis.train.m.txt\", \"F\": \"assets/tennis.train.f.txt\"}\n",
    "\n",
    "# Open the original file for reading\n",
    "with open(genders_train_file , 'r') as file:\n",
    "    male_sentences = []\n",
    "    female_sentences = []\n",
    "\n",
    "    # Iterate through each line in the file\n",
    "    for line in file:\n",
    "        # Split the line into class and sentence based on the tab character\n",
    "        cls, sentence = line.strip().split('\\t')\n",
    "\n",
    "        # Check the class and append the sentence to the corresponding list\n",
    "        # after converting it to lowercase\n",
    "        if cls == 'M':\n",
    "            male_sentences.append(sentence.lower())\n",
    "        elif cls == 'F':\n",
    "            female_sentences.append(sentence.lower())\n",
    "\n",
    "# Write male sentences to a new file\n",
    "with open(genders_files[\"M\"], \"w\") as male_file:\n",
    "    for sentence in male_sentences:\n",
    "        male_file.write(sentence + '\\n')\n",
    "\n",
    "# Write female sentences to a new file\n",
    "with open(genders_files[\"F\"], \"w\") as female_file:\n",
    "    for sentence in female_sentences:\n",
    "        female_file.write(sentence + '\\n')\n",
    "\n",
    "\n",
    "for order in [0, 2, 4]:\n",
    "    models = {\"char\": None, \"word\": None}\n",
    "    models[\"char\"] = {gender: lm.train_char_lm(gender_file, order) for gender, gender_file in genders_files.items()}\n",
    "    models[\"word\"] = {gender: lm.train_word_lm(gender_file, order) for gender, gender_file in genders_files.items()}\n",
    "\n",
    "    print(f\"Order {order}\")\n",
    "    res_gender = {\"char\": {gender:[] for gender in genders}, \"word\": {gender:[] for gender in genders}}\n",
    "    with open(genders_test_file) as test_file:\n",
    "        print(f\"Testing...\")\n",
    "        for i, line in enumerate(tqdm(test_file, total=8214)):\n",
    "            gender, text = line.split('\\t')\n",
    "            for mode in [\"char\", \"word\"]:\n",
    "                scores = {gender: lm.smoothed_perplexity(text, models[mode][gender], order, mode=mode) for gender in genders}\n",
    "                predicted_gender = min(scores, key=scores.get)\n",
    "                if i == 0:\n",
    "                    print(\"---\", \"first line\", \"---\")\n",
    "                    print(f\"{mode} model:\")\n",
    "                    for gender in scores:\n",
    "                        print(f\"{gender:<3}-> {scores[gender]:10.2f}\")\n",
    "                    print(\"------------------\")\n",
    "                # Compare predicted_language with the actual language and count accuracy\n",
    "                res_gender[mode][gender].append(predicted_gender == gender)\n",
    "\n",
    "    for mode in [\"char\", \"word\"]:\n",
    "        print(f\"Accuracy of {mode} model:\")\n",
    "        for gender in genders:\n",
    "            print(f\"{gender} -> {sum(res_gender[mode][gender]) / len(res_gender[mode][gender]):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "| Model Type | n-gram Order | M    | F    |\n",
    "|------------|--------------|------|------|\n",
    "| Char       | 0            | 0.45 | 0.61 |\n",
    "| Char       | 2            | 0.58 | 0.71 |\n",
    "| Char       | 4            | 0.64 | 0.60 |\n",
    "| Word       | 0            | 0.57 | 0.68 |\n",
    "| Word       | 2            | 0.63 | 0.50 |\n",
    "| Word       | 4            | 0.71 | 0.33 |\n",
    "\n",
    "### Analysis\n",
    "\n",
    "1. N-gram Order 0:\n",
    "Char model favors 'F' (0.61) over 'M' (0.45).\n",
    "Word model also leans towards 'F' (0.68) compared to 'M' (0.57).\n",
    "2. N-gram Order 2:\n",
    "Char model improves for both genders with 'F' leading (0.71 vs 0.58).\n",
    "Word model now prefers 'M' (0.63) over 'F' (0.50).\n",
    "3. N-gram Order 4:\n",
    "Char model shows 'M' and 'F' near parity (0.64 vs 0.60).\n",
    "Word model heavily favors 'M' (0.71) with a significant drop for 'F' (0.33).\n",
    "\n",
    "### Inferences\n",
    "1. Word model displays varying biases for 'M' and 'F' as n-gram order changes, possibly suggesting overfitting.\n",
    "2. Char model gives more consistent results between genders but might lack nuances captured by the word model.\n",
    "3. Higher n-gram orders don't guarantee better accuracy, as seen with 'F' in the word model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "belows are the code that is implemtened in the charlm.py file\n",
    "\n",
    "\n",
    "```python\n",
    "def train_word_lm(fname, order=1):\n",
    "    \"\"\"Train a word-level language model using n-grams from a given file.\n",
    "\n",
    "    Args:\n",
    "    - fname (str): Name of the file containing training data.\n",
    "    - order (int): The n-gram order for the language model.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The trained language model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open and read the file content\n",
    "    with open(fname, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Tokenize the data into sentences\n",
    "    sentences = sent_tokenize(data)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    sents = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    lm = defaultdict(Counter)\n",
    "    \n",
    "    # Use a special padding word \"<PAD>\" to pad the sentences.\n",
    "    # This helps in managing the start of sentences.\n",
    "    pad = [\"<PAD>\"] * order\n",
    "    for sent in sents:\n",
    "        # Add padding to the start and a special \"<END>\" token to the end of each sentence\n",
    "        sent = pad + sent + [\"<END>\"]\n",
    "        \n",
    "        # Loop through each word in the sentence and create n-grams\n",
    "        for i in range(len(sent) - order):\n",
    "            history, word = tuple(sent[i:i+order]), sent[i+order]\n",
    "            # Update the counts for this n-gram in the language model\n",
    "            lm[history][word] += 1\n",
    "\n",
    "    # Normalize the counts to get probabilities\n",
    "    outlm = {hist: normalize(chars) for hist, chars in lm.items()}\n",
    "    \n",
    "    return outlm\n",
    "\n",
    "\n",
    "def perplexity(text, lm, order=4, mode=\"char\"):\n",
    "    \"\"\"Compute the perplexity of a given text using an input language model (LM).\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text for which perplexity is calculated.\n",
    "    - lm (dict): The language model.\n",
    "    - order (int): The n-gram order.\n",
    "    - mode (str): Mode of operation - either 'char' for character-level or 'word' for word-level.\n",
    "\n",
    "    Returns:\n",
    "    - float: The perplexity of the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose padding based on mode (character or word)\n",
    "    if mode == \"char\":\n",
    "        pad = \"~\" * order\n",
    "        data = pad + text\n",
    "    elif mode == \"word\":\n",
    "        pad = [\"<PAD>\"] * order\n",
    "        data = pad + word_tokenize(text) + [\"<END>\"]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'char' or 'word'.\")\n",
    "\n",
    "    log_prob = 0\n",
    "    for i in range(len(data) - order):\n",
    "        if mode == \"char\":\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "        elif mode == \"word\":\n",
    "            history, char = tuple(data[i:i+order]), data[i+order]\n",
    "        \n",
    "        # Check if the character or word is in the LM for the given history\n",
    "        if char in [ch for ch, _ in lm[history]]:\n",
    "            prob = dict(lm[history])[char]\n",
    "            log_prob += log(prob)\n",
    "        else:\n",
    "            # Return infinity if probability isn't found in the model\n",
    "            return float(\"inf\")\n",
    "            \n",
    "    return exp(-log_prob / len(data))\n",
    "\n",
    "\n",
    "def smoothed_perplexity(text, lm, order=4, mode=\"char\"):\n",
    "    \"\"\"Compute the smoothed perplexity of a given text using an input LM.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text for which perplexity is calculated.\n",
    "    - lm (dict): The language model.\n",
    "    - order (int): The n-gram order.\n",
    "    - mode (str): Mode of operation - either 'char' for character-level or 'word' for word-level.\n",
    "\n",
    "    Returns:\n",
    "    - float: The smoothed perplexity of the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Choose padding based on mode (character or word)\n",
    "    if mode == \"char\":\n",
    "        pad = \"~\" * order\n",
    "        data = pad + text\n",
    "    elif mode == \"word\":\n",
    "        pad = [\"<PAD>\"] * order\n",
    "        data = pad + word_tokenize(text) + [\"<END>\"]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'char' or 'word'.\")\n",
    "\n",
    "    log_prob = 0\n",
    "    for i in range(len(data) - order):\n",
    "        if mode == \"char\":\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "        elif mode == \"word\":\n",
    "            history, char = tuple(data[i:i+order]), data[i+order]\n",
    "        \n",
    "        try:\n",
    "            prob = dict(lm[history])[char]\n",
    "        except KeyError:\n",
    "            prob = 1.0e-7\n",
    "        log_prob += log(prob)\n",
    "            \n",
    "    return exp(-log_prob / len(data))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
